import os
import torch
import torch.nn.functional as F
import re
import grpc
from grpc_health.v1 import health, health_pb2, health_pb2_grpc
import numpy as np
import threading
from concurrent import futures
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer, util
from llama_cpp import Llama
from huggingface_hub import hf_hub_download

import mutr_analysis_pb2
import mutr_analysis_pb2_grpc

server_port = int(os.getenv("AI_SERVER_PORT", "50051"))
n_ctx = int(os.getenv("n_ctx", "1024"))
n_threads = int(os.getenv("n_threads", "6"))
n_gpu_layers = int(os.getenv("n_gpu_layers", "18"))
n_batch = int(os.getenv("n_batch", "512"))

class MUTRModelEngine:
    def __init__(self):
        self.device = torch.device("cpu")

        # 1. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ë¡œë“œ (Bllossom Llama-3.2-3B)
        # ì™¸êµ­ì–´ ìœ ì¶œ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.
        self.llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx, 
            n_threads=n_threads,
            n_gpu_layers=n_gpu_layers,
            n_batch=n_batch,
            verbose=False
        )
        self.llm_lock = threading.Lock()
        
        # 2. ê°ì • ë¶„ì„ (RoBERTa)
        self.sent_tokenizer = AutoTokenizer.from_pretrained("Seonghaa/korean-emotion-classifier-roberta")
        self.sent_model = AutoModelForSequenceClassification.from_pretrained("Seonghaa/korean-emotion-classifier-roberta").to(self.device)
        
        # 3. ë³€ì¡° ë¶„ì„ (KR-SBERT)
        self.mut_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS").to(self.device)

        self.sent_model.eval()
        print(f"âœ… MUTR Bllossom ì—”ì§„ ë¡œë“œ ì™„ë£Œ (í•œêµ­ì–´ ìµœì í™” ë²„ì „)")

        self.emotion_map = {
            "ê¸°ì¨": "joy", "ë‹¹í™©": "embarrassed", "ë¶„ë…¸": "anger",
            "ë¶ˆì•ˆ": "anxiety", "ìƒì²˜": "hurt", "ìŠ¬í””": "sadness", "í‰ì˜¨": "neutral"
        }

    def _calibrate_mutation(self, similarity):
        """ìœ ì‚¬ë„ë¥¼ ë³€ì¡° ì ìˆ˜ë¡œ ë³´ì •"""
        if similarity >= 0.35: score = (1.0 - similarity) * (0.2 / 0.65)
        elif similarity >= 0.15: score = 0.7 - (similarity - 0.15) * (0.4 / 0.2)
        else: score = 1.0 - max(0, similarity)
        return round(max(0.0, min(1.0, score)), 4)
    
    def get_final_topic(self, gen_topic, baseline_topic):
        """
        Bllossom ëª¨ë¸ ì¶œë ¥ì— ë§ì¶° ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸ ì •ì œ ë¡œì§
        """
        # ë¶ˆí•„ìš”í•œ ì„œìˆ ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        step1 = re.sub(r"^\d+\.\s*", "", gen_topic)
        step1 = step1.replace("'", "").replace("\"", "")
        step1 = step1.replace(".", "").replace("ì œëª©:", "").replace("ì œëª©", "").strip()
        
        # ìµœì¢… Fallback: ìƒì„± ì‹¤íŒ¨ ì‹œ ì´ì „ ìš”ì•½ ìœ ì§€
        if not step1:
            final_topic = baseline_topic if baseline_topic else "ì˜¤ëŠ˜ì˜ ê¸°ë¡"
        else:
            final_topic = step1

        return final_topic

    def _analyze_emotion_internal(self, content):
        sent_inputs = self.sent_tokenizer(content, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            sent_outputs = self.sent_model(**sent_inputs)
            sent_probs = F.softmax(sent_outputs.logits, dim=-1)
            conf, pred = torch.max(sent_probs, dim=-1)
            raw_emotion = self.sent_model.config.id2label[pred.item()]
            emotion_label = self.emotion_map.get(raw_emotion, raw_emotion)
            return emotion_label, float(conf.item())

    def _analyze_mutation_internal(self, content, parent_topic):
        if not parent_topic or not parent_topic.strip():
            return 0.0
        embeddings = self.mut_model.encode([parent_topic, content], convert_to_tensor=True)
        raw_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
        mutation_score = self._calibrate_mutation(raw_sim)
        return mutation_score

    def analyze(self, content, parent_topic, baseline_topic, full_context):
        # [STEP 1] ê°ì • ë¶„ì„
        emotion_label, confidence = self._analyze_emotion_internal(content)

        # [STEP 2] ë³€ì¡° ë¶„ì„
        mutation_score = self._analyze_mutation_internal(content, parent_topic)

        # [STEP 3] LLM ì‹¤í–‰
        prompt = (
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"ë‹¹ì‹ ì€ ê¸°ë¡ì˜ íë¦„ì„ ë¶„ì„í•˜ëŠ” ì„œì‚¬ ì „ë¬¸ê°€ 'ì„±ë‹¨'ì…ë‹ˆë‹¤. "
            f"ì œê³µëœ 'ê³¼ê±° ê¸°ì¤€'ê³¼ 'ìµœê·¼ ì„œì‚¬ íë¦„'ì„ ëŒ€ì¡°í•˜ì—¬, ì´ ì´ì•¼ê¸°ê°€ í˜„ì¬ ì–´ë–¤ ìƒíƒœì— ë„ë‹¬í–ˆëŠ”ì§€ í¬ì°©í•´ ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”.\n\n"
            f"**ë¶„ì„ ì „ëµ:**\n"
            f"1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            f"2. ìµœê·¼ ì„œì‚¬ íë¦„ì´ ì „ì²´ ì„œì‚¬ì—ì„œ ê°–ëŠ” 'ìµœì¢…ì ì¸ ì˜ë¯¸'ë¥¼ ì œëª©ì— ë°˜ì˜í•˜ì„¸ìš”.\n"
            f"3. ê³¼ê±°ì˜ ì£¼ì œì—ì„œ ì–¼ë§ˆë‚˜ ë©€ì–´ì¡ŒëŠ”ì§€, í˜¹ì€ ì–´ë–»ê²Œ ì´ì–´ì§€ëŠ”ì§€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n"
            f"4. ëª…ì‚¬í˜•ìœ¼ë¡œ 20ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.<|eot_id|>\n"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"â— ê³¼ê±° ê¸°ì¤€ (Baseline): {baseline_topic}\n\n"
            f"â— ìµœê·¼ ì„œì‚¬ íë¦„ (Flow):\n"
            f"{full_context}\n\n"
            f"ì§€ì‹œ: ê³¼ê±°ë¡œë¶€í„° ì´ì–´ì§„ ì„œì‚¬ê°€ í˜„ì¬ì˜ íë¦„ ëì—ì„œ ì–´ë–¤ ëª¨ìŠµìœ¼ë¡œ ë³€ëª¨í–ˆëŠ”ì§€ í•œê¸€ ì œëª©ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.<|eot_id|>\n"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            f"ì œëª©: "
        )
        
        with self.llm_lock:
            response = self.llm(
                prompt,
                max_tokens=30,
                temperature=0.0,
                repeat_penalty=1.2,   # í•œêµ­ì–´ ë°˜ë³µ ë°©ì§€ ë° ì¼ê´€ì„± ê°•í™”
                top_p=0.9,
                top_k=40,
                stop=["\n", "ì œëª©:", "<|eot_id|>"]
            )
        
        gen_topic = response['choices'][0]['text'].strip()
        final_topic = self.get_final_topic(gen_topic, baseline_topic)

        return final_topic, emotion_label, confidence, mutation_score

class MUTRAnalysisServicer(mutr_analysis_pb2_grpc.AnalysisServiceServicer):
    def __init__(self):
        self.engine = MUTRModelEngine()

    def AnalyzeNode(self, request, context):
        topic, emotion, conf, mut = self.engine.analyze(
            request.content, request.parent_topic, request.baseline_topic, request.full_context
        )
        return mutr_analysis_pb2.AnalysisResponse(
            topic=topic, emotion=emotion, confidence=conf, mutation_score=mut
        )

def download_model():
    # 1. ì„¤ì •
    repo_id = "Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M"
    filename = "llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf"
    local_dir = "./models"

    # 2. ê²½ë¡œ í™•ì¸ ë° ìƒì„±
    if not os.path.exists(local_dir):
        os.makedirs(local_dir)

    target_path = os.path.join(local_dir, filename)

    # 3. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(target_path):
        print(f"ğŸš€ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {filename}")
        path = hf_hub_download(
            repo_id=repo_id,
            filename=filename,
            local_dir=local_dir
        )
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {path}")
    else:
        print(f"ğŸ“¦ ì´ë¯¸ ëª¨ë¸ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {target_path}")

    return target_path

def serve():
    # 1. ì„œë²„ ê°ì²´ ìƒì„±
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))

    # 2. í—¬ìŠ¤ì²´í¬ ì„œë¹„ìŠ¤ ë“±ë¡
    health_servicer = health.HealthServicer()
    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)

    # ì´ˆê¸° ìƒíƒœ ì„¤ì •: ì•„ì§ ëª¨ë¸ ë¡œë”© ì „ì´ë¯€ë¡œ NOT_SERVING
    health_servicer.set("", health_pb2.HealthCheckResponse.NOT_SERVING)

    # 3. í¬íŠ¸ ì„¤ì • ë° ì„œë²„ ì‹œì‘ (ì´ ì‹œì ë¶€í„° ì™¸ë¶€ì—ì„œ í•‘ì„ ë³´ë‚¼ ìˆ˜ ìˆìŒ)
    server.add_insecure_port(f"[::]:{server_port}")
    print(f"ğŸš€ gRPC ì„œë²„ê°€ í¬íŠ¸ {server_port}ì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ëª¨ë¸ ë¡œë”© ëŒ€ê¸° ì¤‘)")
    server.start()

    # 4. ì‹¤ì œ ë¬´ê±°ìš´ ëª¨ë¸ ì—”ì§„ ë¡œë“œ (ì´ê²Œ ì‚¬ìš©ìë‹˜ì˜ load_model ì—­í• ì…ë‹ˆë‹¤)
    # MUTRAnalysisServicerê°€ ìƒì„±ë  ë•Œ ë‚´ë¶€ì—ì„œ MUTRModelEngineì„ ë§Œë“¤ë©° ëª¨ë¸ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    print("â³ AI ëª¨ë¸ ì—”ì§„ ë¡œë”© ì‹œì‘...")
    servicer = MUTRAnalysisServicer() 
    mutr_analysis_pb2_grpc.add_AnalysisServiceServicer_to_server(servicer, server)

    # 5. ë¡œë”© ì™„ë£Œ í›„ ìƒíƒœ ë³€ê²½: ì´ì œ SERVING
    print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ì„œë¹„ìŠ¤ ì‹œì‘!")
    health_servicer.set("", health_pb2.HealthCheckResponse.SERVING)

    server.wait_for_termination()

if __name__ == "__main__":
    model_path = download_model()
    serve()