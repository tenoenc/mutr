import torch
import torch.nn.functional as F
import re
import grpc
import numpy as np
import time
import threading
from concurrent import futures
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer, util
from llama_cpp import Llama

import mutr_analysis_pb2
import mutr_analysis_pb2_grpc

class MUTRModelEngine:
    def __init__(self):
        self.device = torch.device("cpu")
        
        # 1. ì£¼ì œ ì¶”ì¶œ (Llama-3.2-3B) - [KoBART ëŒ€ì²´ ë° ê³ ë„í™”]
        self.llm = Llama(
            model_path="./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            n_ctx=1024, 
            n_threads=6, 
            n_gpu_layers=18,
            n_batch=512,
            verbose=True
        )
        self.llm_lock = threading.Lock()
        
        # 2. ê°ì • ë¶„ì„ (RoBERTa)
        self.sent_tokenizer = AutoTokenizer.from_pretrained("Seonghaa/korean-emotion-classifier-roberta")
        self.sent_model = AutoModelForSequenceClassification.from_pretrained("Seonghaa/korean-emotion-classifier-roberta").to(self.device)
        
        # 3. ë³€ì¡° ë¶„ì„ (KR-SBERT)
        self.mut_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS").to(self.device)

        self.sent_model.eval()
        print(f"âœ… MUTR ê³ ë„í™” ì—”ì§„ ë¡œë“œ ì™„ë£Œ (Llama 3.2 í†µí•© ë²„ì „)")

        self.emotion_map = {
            "ê¸°ì¨": "joy", "ë‹¹í™©": "embarrassed", "ë¶„ë…¸": "anger",
            "ë¶ˆì•ˆ": "anxiety", "ìƒì²˜": "hurt", "ìŠ¬í””": "sadness", "í‰ì˜¨": "neutral"
        }

    def _calibrate_mutation(self, similarity):
        """ìœ ì‚¬ë„ë¥¼ ë³€ì¡° ì ìˆ˜ë¡œ ë³´ì •"""
        if similarity >= 0.35: score = (1.0 - similarity) * (0.2 / 0.65)
        elif similarity >= 0.15: score = 0.7 - (similarity - 0.15) * (0.4 / 0.2)
        else: score = 1.0 - max(0, similarity)
        return round(max(0.0, min(1.0, score)), 4)
    
    import re

    def get_final_topic(self, gen_topic, parent_summary):
        """
        LLMì´ ìƒì„±í•œ ì œëª©(gen_topic)ì„ ì •ì œí•˜ì—¬ ìµœì¢… ì œëª©(final_topic)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        # 1. ìˆ«ì íŒ¨í„´(ì˜ˆ: "1. ì œëª©") ë° ë§ˆì¹¨í‘œ ì œê±°
        step1 = re.sub(r"^\d+\.\s*", "", gen_topic).replace(".", "").strip()
        
        # 2. ì˜¤ì—¼ëœ ë‹¨ì–´ í•„í„°ë§ (ì™¸êµ­ì–´ê°€ í¬í•¨ëœ ì–´ì ˆ ì‚­ì œ)
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìê°€ í•˜ë‚˜ë¼ë„ ì„ì¸ ë‹¨ì–´ ë©ì–´ë¦¬ë¥¼ í†µì§¸ë¡œ ì œê±°
        pattern = r'\s?\S*[^ê°€-í£a-zA-Z0-9\s]\S*'
        step2 = re.sub(pattern, '', step1).strip()
        
        # 'ì œëª©:' ë¬¸êµ¬ê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        step2 = step2.replace("ì œëª©", "").strip()

        # 3. ìµœì¢… Fallback
        # í•„í„°ë§ í›„ ê²°ê³¼ê°€ ë¹„ì–´ìˆë‹¤ë©´, LLMì´ ì‹¤íŒ¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì´ì „ ìš”ì•½ì„ ìœ ì§€í•¨
        if not step2:
            final_topic = parent_summary if parent_summary else "ì˜¤ëŠ˜ì˜ ê¸°ë¡"
        else:
            final_topic = step2

        # UI ê°€ë…ì„±ì„ ìœ„í•œ ìµœì¢… ê¸¸ì´ ì œí•œ
        return final_topic[:20]

    def analyze(self, content, parent_summary, full_context):
        # [STEP 1] ê°ì • ë¶„ì„
        sent_inputs = self.sent_tokenizer(content, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            sent_outputs = self.sent_model(**sent_inputs)
            sent_probs = F.softmax(sent_outputs.logits, dim=-1)
            conf, pred = torch.max(sent_probs, dim=-1)
            raw_emotion = self.sent_model.config.id2label[pred.item()]
            emotion_label = self.emotion_map.get(raw_emotion, raw_emotion)

        # [STEP 2] ì£¼ì œ ì¶”ì¶œ (Llama 3.2 3B)
        summary_input = f"{full_context} {content}".strip()
        target_context = summary_input[-500:].strip() # ìµœì‹  500ì ë§¥ë½

        prompt = (
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"You are 'Seongdan', an expert in narrative evolution. "
            f"Your task is to detect the 'Mutation' in the user's life and create a Korean title.\n\n"
            f"**ANALYSIS STEPS:**\n"
            f"1. Compare the 'Past Summary' with the 'Current Entry'.\n"
            f"2. If the topic or emotion has changed (Mutation), create a title reflecting the **NEW** direction.\n"
            f"3. If it's a continuation, create a title that deepens the existing theme.\n\n"
            f"**STRICT RULES:**\n"
            f"- Use ONLY Korean (Hangul). No Thai, No Vietnamese.\n"
            f"- Output ONLY a nominal phrase (e.g., 'ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì´ë³„', 'ìƒˆë¡œìš´ í¬ë§ì˜ ì‹œì‘').<|eot_id|>\n"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"â— Past Summary (The baseline): {parent_summary}\n"
            f"â— Recent Context: {target_context}\n"
            f"â— Current Entry (The latest change): {content}\n\n"
            f"Instruction: Observe the flow and generate a title that captures the current state of this narrative.<|eot_id|>\n"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            f"ì œëª©: "
        )
        with self.llm_lock:
            response = self.llm(
                prompt,
                max_tokens=30,
                temperature=0.0,      # ì°½ì˜ì„±ë³´ë‹¤ëŠ” ì •í™•ë„ ìš°ì„ 
                repeat_penalty=1.1,   # ì™¸êµ­ì–´ íƒˆì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                top_p=0.9,            # ìƒìœ„ í™•ë¥  í† í° ì§‘ì¤‘
                top_k=40,             # í›„ë³´êµ°ì„ í•œêµ­ì–´ ìœ„ì£¼ë¡œ ì¢í˜
                stop=["\n", "ì œëª©:", "<|eot_id|>"]
            )
        
        gen_topic = response['choices'][0]['text'].strip()
        final_topic = self.get_final_topic(gen_topic, parent_summary)

        # [STEP 3] ë³€ì¡° ì ìˆ˜ ê³„ì‚°
        mutation_score = 0.0
        if parent_summary and parent_summary.strip():
            embeddings = self.mut_model.encode([parent_summary, content], convert_to_tensor=True)
            raw_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
            mutation_score = self._calibrate_mutation(raw_sim)

        return final_topic, emotion_label, float(conf.item()), float(mutation_score)

class MUTRAnalysisServicer(mutr_analysis_pb2_grpc.AnalysisServiceServicer):
    def __init__(self):
        self.engine = MUTRModelEngine()

    def AnalyzeNode(self, request, context):
        topic, emotion, conf, mut = self.engine.analyze(
            request.content, request.parent_summary, request.full_context
        )
        return mutr_analysis_pb2.AnalysisResponse(
            topic=topic, emotion=emotion, confidence=conf, mutation_score=mut
        )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mutr_analysis_pb2_grpc.add_AnalysisServiceServicer_to_server(MUTRAnalysisServicer(), server)
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ MUTR AI Engine (Verified) started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == "__main__":
    serve()