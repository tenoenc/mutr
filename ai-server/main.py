import torch
import torch.nn.functional as F
import re
import grpc
import numpy as np
from concurrent import futures
from transformers import AutoTokenizer, AutoModelForSequenceClassification, BartForConditionalGeneration, GPT2LMHeadModel, PreTrainedTokenizerFast
from sentence_transformers import SentenceTransformer, util
from kobart import get_kobart_tokenizer

# gRPC ìƒì„± íŒŒì¼
import mutr_analysis_pb2
import mutr_analysis_pb2_grpc

class MUTRModelEngine:
    """MUTR AI ëª¨ë¸ë“¤ì„ ê´€ë¦¬í•˜ê³  ê³ ë„í™”ëœ ê°€ë“œë ˆì¼ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ì§„"""
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # 1. ì£¼ì œ ì¶”ì¶œ (KoBART-title)
        self.kb_tokenizer = get_kobart_tokenizer()
        self.kb_model = BartForConditionalGeneration.from_pretrained("EbanLee/kobart-title").to(self.device)
        
        # 2. ê°ì • ë¶„ì„ (RoBERTa)
        self.sent_tokenizer = AutoTokenizer.from_pretrained("Seonghaa/korean-emotion-classifier-roberta")
        self.sent_model = AutoModelForSequenceClassification.from_pretrained("Seonghaa/korean-emotion-classifier-roberta").to(self.device)
        
        # 3. ë³€ì¡° ë° ì˜ë¯¸ ìœ ì‚¬ë„ ë¶„ì„ (KR-SBERT)
        self.mut_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS").to(self.device)

        # 4. í’ˆì§ˆ ê²€ì¦ ê°€ë“œë ˆì¼ (KoGPT2)
        self.ppl_tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
        self.ppl_model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2").to(self.device)
        
        self.kb_model.eval()
        self.sent_model.eval()
        self.ppl_model.eval()
        print(f"âœ… MUTR ê³ ë„í™” ì—”ì§„ ë¡œë“œ ì™„ë£Œ ({self.device})")

    def _calculate_ppl(self, text):
        """ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€(Perplexity) ê³„ì‚°"""
        if not text or len(text.strip()) < 1: return 999999
        encodings = self.ppl_tokenizer(text, return_tensors="pt")
        input_ids = encodings.input_ids.to(self.device)
        with torch.no_grad():
            outputs = self.ppl_model(input_ids, labels=input_ids)
            ppl = np.exp(outputs.loss.item())
        return ppl if not (np.isnan(ppl) or np.isinf(ppl)) else 999999

    def _validate_by_ai(self, content, generated, ppl_score, sim_score):
        """
        [AI ìˆ˜ì¹˜ ê¸°ë°˜ ì—„ê²©í•œ í’ˆì§ˆ ê²€ì¦]
        1. ì˜ë¯¸ ìœ ì‚¬ë„: ì´ˆë‹¨ë¬¸ ì…ë ¥ ì‹œ ê¸°ì¤€ ìƒí–¥ (0.65) í•˜ì—¬ í™˜ê° ì°¨ë‹¨
        2. ê°€ë³€ PPL: 15ì ë¯¸ë§Œ í•µì‹¬ ìš”ì•½ ë³´í˜¸ (ì„ê³„ê°’ ì™„í™”)
        3. ë°˜ë³µì„±: 10ì ì´ìƒ ë¬¸ì¥ì—ì„œ ë¬¸ì ë°˜ë³µë¥  ê²€ì‚¬
        """
        # (1) ë™ì  ìœ ì‚¬ë„ ì„ê³„ê°’: ì…ë ¥ì´ ì§§ì„ìˆ˜ë¡ ë” ì—„ê²©í•˜ê²Œ ê²€ì¦ (í™˜ê° ë°©ì–´)
        sim_threshold = 0.65 if len(content) <= 5 else 0.38
        if sim_score < sim_threshold:
            return False, "SEMANTIC_MISMATCH"
        
        # (2) ê°€ë³€ PPL ì„ê³„ê°’: ì§§ì€ í•µì‹¬ ìš”ì•½(ìš”ê°€, ì¡¸ì—…ì‹ ë“±)ì˜ ê³¼ì‰ ì§„ì•• ë°©ì§€
        ppl_threshold = 100000 if len(generated.replace(" ", "")) < 15 else 350
        if ppl_score > ppl_threshold:
            return False, "UNNATURAL_PPL"
            
        # (3) ë°˜ë³µì„± ê²€ì‚¬ ë³´ì •: ê³µë°±/ê¸°í˜¸ ì œì™¸, 10ì ì´ìƒì—ì„œë§Œ ì‘ë™
        if len(generated) > 10:
            pure_gen = re.sub(r"[^\w]", "", generated) 
            if len(pure_gen) > 0:
                for char in set(pure_gen):
                    if pure_gen.count(char) / len(pure_gen) > 0.35:
                        return False, "REPETITIVE_ARTIFACT"

        return True, "PASS"

    def _format_clean(self, text):
        """ë‰´ìŠ¤ í•„í„°ë§ì´ ì•„ë‹Œ ì¶œë ¥ 'í˜•ì‹' ì •ì œ (ëŒ€ê´„í˜¸ ì œê±°)"""
        return re.sub(r"\[.*?\]", "", text).strip()

    def _calibrate_mutation(self, similarity):
        """ìœ ì‚¬ë„ë¥¼ MUTR ë³€ì¡° ì ìˆ˜(0~1)ë¡œ ë³´ì •"""
        if similarity >= 0.35: score = (1.0 - similarity) * (0.2 / 0.65)
        elif similarity >= 0.15: score = 0.7 - (similarity - 0.15) * (0.4 / 0.2)
        else: score = 1.0 - max(0, similarity)
        return round(max(0.0, min(1.0, score)), 4)

    def analyze(self, content, parent_summary, full_context):
        # STEP 1. ê°ì • ë¶„ì„ (í˜„ì¬ ë…¸ë“œ ê¸€ ë‹¨ë…)
        sent_inputs = self.sent_tokenizer(content, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            sent_outputs = self.sent_model(**sent_inputs)
            sent_probs = F.softmax(sent_outputs.logits, dim=-1)
            conf, pred = torch.max(sent_probs, dim=-1)
            emotion_label = self.sent_model.config.id2label[pred.item()]

        # STEP 2. ì£¼ì œ ì¶”ì¶œ (ì „ì²´ ë§¥ë½ + í˜„ì¬ ê¸€)
        summary_input = f"{full_context} {content}".strip()
        kb_inputs = self.kb_tokenizer(summary_input, return_tensors="pt", truncation=True, max_length=1024).to(self.device)
        with torch.no_grad():
            summary_ids = self.kb_model.generate(
                input_ids=kb_inputs['input_ids'], max_length=40, num_beams=4,
                repetition_penalty=4.5, no_repeat_ngram_size=2
            )
        generated_raw = self.kb_tokenizer.decode(summary_ids[0], skip_special_tokens=True).strip()
        cleaned_topic = self._format_clean(generated_raw)

        # STEP 3. AI ê¸°ë°˜ ê°€ë“œë ˆì¼ ê²€ì¦
        ppl = self._calculate_ppl(cleaned_topic)
        # í˜„ì¬ ë…¸ë“œ ë‚´ìš©ê³¼ ìš”ì•½ë¬¸ì˜ ì˜ë¯¸ ìœ ì‚¬ë„ ì¸¡ì •
        sim_val = util.cos_sim(self.mut_model.encode(content), self.mut_model.encode(cleaned_topic)).item()
        
        is_pass, _ = self._validate_by_ai(content, cleaned_topic, ppl, sim_val)
        # í’ˆì§ˆ ë¯¸ë‹¬ ì‹œ ì›ë¬¸ ì•ë¶€ë¶„ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ Fallback
        final_topic = cleaned_topic if is_pass else content[:15].strip() + "..."

        # STEP 4. ë³€ì¡° ì ìˆ˜ ê³„ì‚° (ì´ì „ ìš”ì•½ vs í˜„ì¬ ê¸€)
        mutation_score = 0.0
        if parent_summary and parent_summary.strip():
            embeddings = self.mut_model.encode([parent_summary, content], convert_to_tensor=True)
            raw_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
            mutation_score = self._calibrate_mutation(raw_sim)

        return final_topic, emotion_label, float(conf.item()), float(mutation_score)

# --- [gRPC ì„œë¹„ìŠ¤ ì •ì˜] ---
class MUTRAnalysisServicer(mutr_analysis_pb2_grpc.AnalysisServiceServicer):
    def __init__(self):
        self.engine = MUTRModelEngine()

    def AnalyzeNode(self, request, context):
        topic, emotion, conf, mut = self.engine.analyze(
            request.content, request.parent_summary, request.full_context
        )
        return mutr_analysis_pb2.AnalysisResponse(
            topic=topic, emotion=emotion, confidence=conf, mutation_score=mut
        )

# --- [ì„œë²„ ì‹¤í–‰ë¶€] ---
def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mutr_analysis_pb2_grpc.add_AnalysisServiceServicer_to_server(MUTRAnalysisServicer(), server)
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ MUTR AI-Guardrail Engine started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == "__main__":
    serve()