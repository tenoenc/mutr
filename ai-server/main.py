import torch
import torch.nn.functional as F
import re
import grpc
import numpy as np
import time
import threading
from concurrent import futures
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer, util
from llama_cpp import Llama

import mutr_analysis_pb2
import mutr_analysis_pb2_grpc

class MUTRModelEngine:
    def __init__(self):
        self.device = torch.device("cpu")
        
        # 1. ì£¼ì œ ì¶”ì¶œ (Llama-3.2-3B) - [KoBART ëŒ€ì²´ ë° ê³ ë„í™”]
        self.llm = Llama(
            model_path="./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            n_ctx=1024, 
            n_threads=6, 
            n_gpu_layers=18,
            n_batch=512,
            verbose=True
        )
        self.llm_lock = threading.Lock()
        
        # 2. ê°ì • ë¶„ì„ (RoBERTa)
        self.sent_tokenizer = AutoTokenizer.from_pretrained("Seonghaa/korean-emotion-classifier-roberta")
        self.sent_model = AutoModelForSequenceClassification.from_pretrained("Seonghaa/korean-emotion-classifier-roberta").to(self.device)
        
        # 3. ë³€ì¡° ë¶„ì„ (KR-SBERT)
        self.mut_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS").to(self.device)

        self.sent_model.eval()
        print(f"âœ… MUTR ê³ ë„í™” ì—”ì§„ ë¡œë“œ ì™„ë£Œ (Llama 3.2 í†µí•© ë²„ì „)")

        self.emotion_map = {
            "ê¸°ì¨": "joy", "ë‹¹í™©": "embarrassed", "ë¶„ë…¸": "anger",
            "ë¶ˆì•ˆ": "anxiety", "ìƒì²˜": "hurt", "ìŠ¬í””": "sadness", "í‰ì˜¨": "neutral"
        }

    def _calibrate_mutation(self, similarity):
        """ìœ ì‚¬ë„ë¥¼ ë³€ì¡° ì ìˆ˜ë¡œ ë³´ì •"""
        if similarity >= 0.35: score = (1.0 - similarity) * (0.2 / 0.65)
        elif similarity >= 0.15: score = 0.7 - (similarity - 0.15) * (0.4 / 0.2)
        else: score = 1.0 - max(0, similarity)
        return round(max(0.0, min(1.0, score)), 4)

    def analyze(self, content, parent_summary, full_context):
        # [STEP 1] ê°ì • ë¶„ì„
        sent_inputs = self.sent_tokenizer(content, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            sent_outputs = self.sent_model(**sent_inputs)
            sent_probs = F.softmax(sent_outputs.logits, dim=-1)
            conf, pred = torch.max(sent_probs, dim=-1)
            raw_emotion = self.sent_model.config.id2label[pred.item()]
            emotion_label = self.emotion_map.get(raw_emotion, raw_emotion)

        # [STEP 2] ì£¼ì œ ì¶”ì¶œ (Llama 3.2 3B)
        summary_input = f"{full_context} {content}".strip()
        target_context = summary_input[-500:].strip() # ìµœì‹  500ì ë§¥ë½

        prompt = (
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"You are 'Seongdan', a professional diary analyzer. "
            f"Your goal is to create a concise, abstract Korean title (nominal phrase) for the current input. "
            f"**CRITICAL RULES:**\n"
            f"1. NEVER copy the dialogue directly from the text.\n"
            f"2. DO NOT use quotation marks or conversational endings (~í•˜ì, ~í–ˆë‹¤).\n"
            f"3. Use abstract nouns to represent the core theme.\n\n"
            f"Examples:\n"
            f"- Input: 'ìš°ë¦¬ í”¼ì ë¨¹ì! ì§„ì§œ ë°°ê³ íŒŒ!' -> Title: í”¼ìë¥¼ í–¥í•œ ê°ˆë§\n"
            f"- Input: '8ì‹œê¹Œì§€ ëª¨ì—¬ì„œ ê²Œì„í•˜ê¸°ë¡œ í•¨' -> Title: ì €ë… ëª¨ì„ ì•½ì†\n"
            f"- Input: 'ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ë‹¤...' -> Title: ë¬´ê¸°ë ¥í•œ ì˜¤í›„ì˜ ê¸°ë¡\n"
            f"<|eot_id|>\n"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"Previous Theme: {parent_summary if len(summary_input) >= 500 else 'None'}\n"
            f"Current Content: {target_context}\n\n"
            f"Title (In Korean):<|eot_id|>\n"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            f"ì œëª©: "
        )
        with self.llm_lock:
            response = self.llm(
                prompt,
                max_tokens=25,
                temperature=0.0,
                repeat_penalty=2.0,
                stop=["\n", "1.", "â—", "ì œëª©:", "<|eot_id|>"]
            )
        
        gen_topic = response['choices'][0]['text'].strip()
        final_topic = re.sub(r"^\d+\.\s*", "", gen_topic).replace(".", "").strip()
        
        # [ì‚¬ìš©ì ì œì•ˆ ì§€ëŠ¥í˜• Fallback]
        if not final_topic:
            final_topic = parent_summary if parent_summary else "ì˜¤ëŠ˜ì˜ ê¸°ë¡"

        # [STEP 3] ë³€ì¡° ì ìˆ˜ ê³„ì‚°
        mutation_score = 0.0
        if parent_summary and parent_summary.strip():
            embeddings = self.mut_model.encode([parent_summary, content], convert_to_tensor=True)
            raw_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
            mutation_score = self._calibrate_mutation(raw_sim)

        return final_topic, emotion_label, float(conf.item()), float(mutation_score)

class MUTRAnalysisServicer(mutr_analysis_pb2_grpc.AnalysisServiceServicer):
    def __init__(self):
        self.engine = MUTRModelEngine()

    def AnalyzeNode(self, request, context):
        topic, emotion, conf, mut = self.engine.analyze(
            request.content, request.parent_summary, request.full_context
        )
        return mutr_analysis_pb2.AnalysisResponse(
            topic=topic, emotion=emotion, confidence=conf, mutation_score=mut
        )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mutr_analysis_pb2_grpc.add_AnalysisServiceServicer_to_server(MUTRAnalysisServicer(), server)
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ MUTR AI Engine (Verified) started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == "__main__":
    serve()