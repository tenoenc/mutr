import torch
import torch.nn.functional as F
import re
import grpc
import numpy as np
import threading
from concurrent import futures
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer, util
from llama_cpp import Llama

import mutr_analysis_pb2
import mutr_analysis_pb2_grpc

class MUTRModelEngine:
    def __init__(self):
        self.device = torch.device("cpu")
        
        # 1. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ ë¡œë“œ (Bllossom Llama-3.2-3B)
        # ì™¸êµ­ì–´ ìœ ì¶œ ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.
        self.llm = Llama(
            model_path="./models/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf",
            n_ctx=1024, 
            n_threads=6, 
            n_gpu_layers=18,
            n_batch=512,
            verbose=True
        )
        self.llm_lock = threading.Lock()
        
        # 2. ê°ì • ë¶„ì„ (RoBERTa)
        self.sent_tokenizer = AutoTokenizer.from_pretrained("Seonghaa/korean-emotion-classifier-roberta")
        self.sent_model = AutoModelForSequenceClassification.from_pretrained("Seonghaa/korean-emotion-classifier-roberta").to(self.device)
        
        # 3. ë³€ì¡° ë¶„ì„ (KR-SBERT)
        self.mut_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS").to(self.device)

        self.sent_model.eval()
        print(f"âœ… MUTR Bllossom ì—”ì§„ ë¡œë“œ ì™„ë£Œ (í•œêµ­ì–´ ìµœì í™” ë²„ì „)")

        self.emotion_map = {
            "ê¸°ì¨": "joy", "ë‹¹í™©": "embarrassed", "ë¶„ë…¸": "anger",
            "ë¶ˆì•ˆ": "anxiety", "ìƒì²˜": "hurt", "ìŠ¬í””": "sadness", "í‰ì˜¨": "neutral"
        }

    def _calibrate_mutation(self, similarity):
        """ìœ ì‚¬ë„ë¥¼ ë³€ì¡° ì ìˆ˜ë¡œ ë³´ì •"""
        if similarity >= 0.35: score = (1.0 - similarity) * (0.2 / 0.65)
        elif similarity >= 0.15: score = 0.7 - (similarity - 0.15) * (0.4 / 0.2)
        else: score = 1.0 - max(0, similarity)
        return round(max(0.0, min(1.0, score)), 4)
    
    def get_final_topic(self, gen_topic, parent_summary):
        """
        Bllossom ëª¨ë¸ ì¶œë ¥ì— ë§ì¶° ê°„ì†Œí™”ëœ í…ìŠ¤íŠ¸ ì •ì œ ë¡œì§
        """
        # ë¶ˆí•„ìš”í•œ ì„œìˆ ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        step1 = re.sub(r"^\d+\.\s*", "", gen_topic)
        step1 = step1.replace("'", "").replace("\"", "")
        step1 = step1.replace(".", "").replace("ì œëª©:", "").replace("ì œëª©", "").strip()
        
        # ìµœì¢… Fallback: ìƒì„± ì‹¤íŒ¨ ì‹œ ì´ì „ ìš”ì•½ ìœ ì§€
        if not step1:
            final_topic = parent_summary if parent_summary else "ì˜¤ëŠ˜ì˜ ê¸°ë¡"
        else:
            final_topic = step1

        return final_topic

    def analyze(self, content, parent_summary, full_context):
        # [STEP 1] ê°ì • ë¶„ì„
        sent_inputs = self.sent_tokenizer(content, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            sent_outputs = self.sent_model(**sent_inputs)
            sent_probs = F.softmax(sent_outputs.logits, dim=-1)
            conf, pred = torch.max(sent_probs, dim=-1)
            raw_emotion = self.sent_model.config.id2label[pred.item()]
            emotion_label = self.emotion_map.get(raw_emotion, raw_emotion)

        # [STEP 2] ì£¼ì œ ì¶”ì¶œ (í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ê³ ë„í™”)
        # ëª¨ë¸ì˜ ëª¨êµ­ì–´ì¸ í•œêµ­ì–´ë¡œ ì§€ì‹œí•˜ì—¬ ë” ì •í™•í•œ ê²°ê³¼ë¬¼ì„ ìœ ë„í•©ë‹ˆë‹¤.
        prompt = (
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"ë‹¹ì‹ ì€ ê¸°ë¡ì˜ íë¦„ì„ ë¶„ì„í•˜ëŠ” ì„œì‚¬ ì „ë¬¸ê°€ 'ì„±ë‹¨'ì…ë‹ˆë‹¤. "
            f"ì œê³µëœ 'ê³¼ê±° ê¸°ì¤€'ê³¼ 'ìµœê·¼ ì„œì‚¬ íë¦„'ì„ ëŒ€ì¡°í•˜ì—¬, ì´ ì´ì•¼ê¸°ê°€ í˜„ì¬ ì–´ë–¤ ìƒíƒœì— ë„ë‹¬í–ˆëŠ”ì§€ í¬ì°©í•´ ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”.\n\n"
            f"**ë¶„ì„ ì „ëµ:**\n"
            f"1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            f"2. ìµœê·¼ ì„œì‚¬ íë¦„ì´ ì „ì²´ ì„œì‚¬ì—ì„œ ê°–ëŠ” 'ìµœì¢…ì ì¸ ì˜ë¯¸'ë¥¼ ì œëª©ì— ë°˜ì˜í•˜ì„¸ìš”.\n"
            f"3. ê³¼ê±°ì˜ ì£¼ì œì—ì„œ ì–¼ë§ˆë‚˜ ë©€ì–´ì¡ŒëŠ”ì§€, í˜¹ì€ ì–´ë–»ê²Œ ì´ì–´ì§€ëŠ”ì§€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.\n"
            f"4. ëª…ì‚¬í˜•ìœ¼ë¡œ 20ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.<|eot_id|>\n"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"â— ê³¼ê±° ê¸°ì¤€ (Baseline): {parent_summary}\n\n"
            f"â— ìµœê·¼ ì„œì‚¬ íë¦„ (Flow):\n"
            f"{full_context}\n\n"
            f"ì§€ì‹œ: ê³¼ê±°ë¡œë¶€í„° ì´ì–´ì§„ ì„œì‚¬ê°€ í˜„ì¬ì˜ íë¦„ ëì—ì„œ ì–´ë–¤ ëª¨ìŠµìœ¼ë¡œ ë³€ëª¨í–ˆëŠ”ì§€ í•œê¸€ ì œëª©ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.<|eot_id|>\n"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            f"ì œëª©: "
        )
        
        with self.llm_lock:
            response = self.llm(
                prompt,
                max_tokens=30,
                temperature=0.0,
                repeat_penalty=1.2,   # í•œêµ­ì–´ ë°˜ë³µ ë°©ì§€ ë° ì¼ê´€ì„± ê°•í™”
                top_p=0.9,
                top_k=40,
                stop=["\n", "ì œëª©:", "<|eot_id|>"]
            )
        
        gen_topic = response['choices'][0]['text'].strip()
        final_topic = self.get_final_topic(gen_topic, parent_summary)

        # [STEP 3] ë³€ì¡° ì ìˆ˜ ê³„ì‚°
        mutation_score = 0.0
        if parent_summary and parent_summary.strip():
            embeddings = self.mut_model.encode([parent_summary, content], convert_to_tensor=True)
            raw_sim = util.cos_sim(embeddings[0], embeddings[1]).item()
            mutation_score = self._calibrate_mutation(raw_sim)

        return final_topic, emotion_label, float(conf.item()), float(mutation_score)

class MUTRAnalysisServicer(mutr_analysis_pb2_grpc.AnalysisServiceServicer):
    def __init__(self):
        self.engine = MUTRModelEngine()

    def AnalyzeNode(self, request, context):
        topic, emotion, conf, mut = self.engine.analyze(
            request.content, request.parent_summary, request.full_context
        )
        return mutr_analysis_pb2.AnalysisResponse(
            topic=topic, emotion=emotion, confidence=conf, mutation_score=mut
        )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    mutr_analysis_pb2_grpc.add_AnalysisServiceServicer_to_server(MUTRAnalysisServicer(), server)
    server.add_insecure_port('[::]:50051')
    print("ğŸš€ MUTR Bllossom AI Engine started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == "__main__":
    serve()